# Transfer Learning Framework for Blood Pressure Prediction
# è¡€å‹é¢„æµ‹è¿ç§»å­¦ä¹ æ¡†æ¶

## Overview / æ¦‚è¿°

This framework implements transfer learning for personalized blood pressure prediction from PPG (PLETH) signals. It significantly improves prediction accuracy by adapting general models to individual patients.

æœ¬æ¡†æ¶å®ç°äº†åŸºäºPPG(PLETH)ä¿¡å·çš„ä¸ªæ€§åŒ–è¡€å‹é¢„æµ‹è¿ç§»å­¦ä¹ ã€‚é€šè¿‡å°†é€šç”¨æ¨¡å‹é€‚é…åˆ°ä¸ªä½“æ‚£è€…,æ˜¾è‘—æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚

### Expected Performance / é¢„æœŸæ€§èƒ½

- **General Model**: MAE ~10-15 mmHg / é€šç”¨æ¨¡å‹: MAEçº¦10-15 mmHg
- **Simple Calibration**: MAE ~8-12 mmHg (may be worse) / ç®€å•æ ¡å‡†: MAEçº¦8-12 mmHg(å¯èƒ½æ›´å·®)
- **Transfer Learning**: MAE ~3-5 mmHg (60-70% improvement) / è¿ç§»å­¦ä¹ : MAEçº¦3-5 mmHg(æ”¹å–„60-70%)

---

## Project Structure / é¡¹ç›®ç»“æ„

```
PAE_NEW/
â”œâ”€â”€ config_transfer.py          # Transfer learning configuration / è¿ç§»å­¦ä¹ é…ç½®
â”œâ”€â”€ data_splitter.py            # Patient data splitter / æ‚£è€…æ•°æ®åˆ†å‰²å™¨
â”œâ”€â”€ transfer_learning.py        # Core transfer learning classes / æ ¸å¿ƒè¿ç§»å­¦ä¹ ç±»
â”œâ”€â”€ main_transfer_learning.py   # Main pipeline / ä¸»ç¨‹åº
â”œâ”€â”€ evaluation.py               # Enhanced with TL visualizations / å¢å¼ºçš„å¯è§†åŒ–
â””â”€â”€ TRANSFER_LEARNING_README.md # This file / æœ¬æ–‡ä»¶
```

---

## Quick Start / å¿«é€Ÿå¼€å§‹

### 1. Installation / å®‰è£…

Make sure you have all dependencies installed:
ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–:

```bash
pip install -r requirements.txt
```

Required packages:
å¿…éœ€çš„åŒ…:
- numpy
- pandas
- matplotlib
- scikit-learn
- xgboost (recommended) / æ¨è
- lightgbm (optional) / å¯é€‰

### 2. Configuration / é…ç½®

Edit `config_transfer.py` to customize:
ç¼–è¾‘ `config_transfer.py` æ¥è‡ªå®šä¹‰:

```python
# Model type / æ¨¡å‹ç±»å‹
GENERAL_MODEL_CONFIG['model_type'] = 'xgboost'  # or 'lightgbm', 'gradient_boosting'

# Calibration data amount / æ ¡å‡†æ•°æ®é‡
DATA_SPLIT_CONFIG['sample_based']['n_samples'] = 200  # heartbeats

# Fine-tuning parameters / å¾®è°ƒå‚æ•°
FINE_TUNING_CONFIG['xgboost']['learning_rate'] = 0.01  # reduced for stability
FINE_TUNING_CONFIG['xgboost']['n_estimators'] = 100    # new trees to add
```

### 3. Run Pipeline / è¿è¡Œæµç¨‹

Basic usage:
åŸºæœ¬ç”¨æ³•:

```bash
python main_transfer_learning.py
```

With custom parameters:
ä½¿ç”¨è‡ªå®šä¹‰å‚æ•°:

```bash
python main_transfer_learning.py \
    --test-file path/to/test.vital \
    --model-type xgboost \
    --calibration-samples 200 \
    --verbose
```

---

## Pipeline Steps / æµç¨‹æ­¥éª¤

The transfer learning pipeline consists of 7 steps:
è¿ç§»å­¦ä¹ æµç¨‹åŒ…å«7ä¸ªæ­¥éª¤:

1. **Load Training Data** / åŠ è½½è®­ç»ƒæ•°æ®
   - Load all training patient files / åŠ è½½æ‰€æœ‰è®­ç»ƒæ‚£è€…æ–‡ä»¶
   - Process signals and extract features / å¤„ç†ä¿¡å·å¹¶æå–ç‰¹å¾
   - Combine into unified dataset / åˆå¹¶ä¸ºç»Ÿä¸€æ•°æ®é›†

2. **Train General Models** / è®­ç»ƒé€šç”¨æ¨¡å‹
   - Train on all training data / åœ¨æ‰€æœ‰è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒ
   - Separate models for systolic and diastolic BP / æ”¶ç¼©å‹å’Œèˆ’å¼ å‹åˆ†åˆ«å»ºæ¨¡

3. **Save General Models** / ä¿å­˜é€šç”¨æ¨¡å‹
   - Save to `saved_models/general_models/` / ä¿å­˜åˆ°é€šç”¨æ¨¡å‹ç›®å½•

4. **Load Test Data** / åŠ è½½æµ‹è¯•æ•°æ®
   - Load and process test patient file / åŠ è½½å¹¶å¤„ç†æµ‹è¯•æ‚£è€…æ–‡ä»¶

5. **Split Test Data** / åˆ†å‰²æµ‹è¯•æ•°æ®
   - Calibration set: First 100-300 heartbeats / æ ¡å‡†é›†: å‰100-300ä¸ªå¿ƒè·³
   - Evaluation set: Remaining heartbeats / è¯„ä¼°é›†: å‰©ä½™å¿ƒè·³

6. **Evaluate General Models** / è¯„ä¼°é€šç”¨æ¨¡å‹
   - Baseline performance on evaluation set / åœ¨è¯„ä¼°é›†ä¸Šçš„åŸºçº¿æ€§èƒ½

7. **Fine-tune and Evaluate** / å¾®è°ƒå¹¶è¯„ä¼°
   - Fine-tune using calibration data / ä½¿ç”¨æ ¡å‡†æ•°æ®å¾®è°ƒ
   - Evaluate personalized models / è¯„ä¼°ä¸ªæ€§åŒ–æ¨¡å‹
   - Generate comparison visualizations / ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–

---

## Output / è¾“å‡º

Results are saved to `results_transfer_learning/[test_file_name]/`:
ç»“æœä¿å­˜åˆ° `results_transfer_learning/[æµ‹è¯•æ–‡ä»¶å]/`:

### Directory Structure / ç›®å½•ç»“æ„

```
results_transfer_learning/
â””â”€â”€ [test_file_name]/
    â”œâ”€â”€ plots/                                    # Visualizations / å¯è§†åŒ–
    â”‚   â”œâ”€â”€ transfer_learning_comparison_Systolic.png
    â”‚   â”œâ”€â”€ transfer_learning_comparison_Diastolic.png
    â”‚   â””â”€â”€ transfer_learning_improvement_summary.png
    â”œâ”€â”€ reports/                                  # Text reports / æ–‡æœ¬æŠ¥å‘Š
    â”‚   â””â”€â”€ transfer_learning_report.txt
    â””â”€â”€ predictions/                              # Prediction results / é¢„æµ‹ç»“æœ
        â””â”€â”€ predictions.csv
```

### Visualizations / å¯è§†åŒ–

1. **Transfer Learning Comparison** (2 files: Systolic & Diastolic)
   è¿ç§»å­¦ä¹ å¯¹æ¯” (2ä¸ªæ–‡ä»¶: æ”¶ç¼©å‹å’Œèˆ’å¼ å‹)
   - Time series comparison / æ—¶é—´åºåˆ—å¯¹æ¯”
   - Scatter plot comparison / æ•£ç‚¹å›¾å¯¹æ¯”
   - Error distribution / è¯¯å·®åˆ†å¸ƒ
   - Performance metrics / æ€§èƒ½æŒ‡æ ‡

2. **Improvement Summary**
   æ”¹å–„æ€»ç»“
   - MAE improvement percentage / MAEæ”¹å–„ç™¾åˆ†æ¯”
   - MSE improvement percentage / MSEæ”¹å–„ç™¾åˆ†æ¯”

### Text Report / æ–‡æœ¬æŠ¥å‘Š

Includes:
åŒ…å«:
- Dataset statistics / æ•°æ®é›†ç»Ÿè®¡
- General model performance / é€šç”¨æ¨¡å‹æ€§èƒ½
- Personalized model performance / ä¸ªæ€§åŒ–æ¨¡å‹æ€§èƒ½
- Improvement percentages / æ”¹å–„ç™¾åˆ†æ¯”
- Target achievement / ç›®æ ‡è¾¾æˆæƒ…å†µ

---

## Key Configuration Parameters / å…³é”®é…ç½®å‚æ•°

### Model Selection / æ¨¡å‹é€‰æ‹©

```python
# config_transfer.py

# Choose model type / é€‰æ‹©æ¨¡å‹ç±»å‹
GENERAL_MODEL_CONFIG['model_type'] = 'xgboost'  # Best performance / æœ€ä½³æ€§èƒ½

# XGBoost parameters / XGBoostå‚æ•°
GENERAL_MODEL_CONFIG['xgboost'] = {
    'n_estimators': 200,        # Number of trees / æ ‘çš„æ•°é‡
    'max_depth': 10,            # Tree depth / æ ‘æ·±åº¦
    'learning_rate': 0.1,       # Learning rate / å­¦ä¹ ç‡
    'subsample': 0.8,           # Data subsampling / æ•°æ®å­é‡‡æ ·
    'reg_alpha': 0.01,          # L1 regularization / L1æ­£åˆ™åŒ–
    'reg_lambda': 1.0,          # L2 regularization / L2æ­£åˆ™åŒ–
}
```

### Fine-tuning Strategy / å¾®è°ƒç­–ç•¥

```python
# Fine-tuning configuration / å¾®è°ƒé…ç½®
FINE_TUNING_CONFIG = {
    'strategy': 'incremental',  # or 'correction_model' / æˆ–'æ ¡æ­£æ¨¡å‹'

    'xgboost': {
        'n_estimators': 100,      # New trees to add / å¢é‡æ·»åŠ çš„æ ‘
        'learning_rate': 0.01,    # Reduced LR / é™ä½çš„å­¦ä¹ ç‡
        'reg_alpha': 0.05,        # Increased regularization / å¢åŠ æ­£åˆ™åŒ–
    },

    'early_stopping': {
        'enabled': True,
        'rounds': 20,
        'validation_fraction': 0.2
    }
}
```

### Data Splitting / æ•°æ®åˆ†å‰²

```python
# Choose split method / é€‰æ‹©åˆ†å‰²æ–¹æ³•
DATA_SPLIT_CONFIG['split_method'] = 'sample_based'  # Recommended / æ¨è

# Sample-based configuration / åŸºäºæ ·æœ¬çš„é…ç½®
DATA_SPLIT_CONFIG['sample_based'] = {
    'n_samples': 200,      # Calibration heartbeats / æ ¡å‡†å¿ƒè·³æ•°
    'min_samples': 100,    # Minimum required / æœ€å°è¦æ±‚
    'max_samples': 300     # Maximum allowed / æœ€å¤§å…è®¸
}

# Alternative: time-based / æ›¿ä»£æ–¹æ¡ˆ: åŸºäºæ—¶é—´
DATA_SPLIT_CONFIG['time_based'] = {
    'duration_minutes': 5   # First 5 minutes for calibration / å‰5åˆ†é’Ÿç”¨äºæ ¡å‡†
}

# Alternative: ratio-based / æ›¿ä»£æ–¹æ¡ˆ: åŸºäºæ¯”ä¾‹
DATA_SPLIT_CONFIG['ratio_based'] = {
    'calibration_ratio': 0.25   # 25% for calibration / 25%ç”¨äºæ ¡å‡†
}
```

---

## Advanced Usage / é«˜çº§ç”¨æ³•

### Experiment with Different Calibration Sizes / å®éªŒä¸åŒçš„æ ¡å‡†é›†å¤§å°

Enable calibration size experiment in `config_transfer.py`:
åœ¨ `config_transfer.py` ä¸­å¯ç”¨æ ¡å‡†å¤§å°å®éªŒ:

```python
EXPERIMENT_CONFIG['calibration_size_experiment'] = {
    'enabled': True,
    'sizes': [100, 150, 200, 250, 300]
}
```

This will test multiple calibration sizes and report the best one.
è¿™å°†æµ‹è¯•å¤šä¸ªæ ¡å‡†å¤§å°å¹¶æŠ¥å‘Šæœ€ä½³çš„ã€‚

### Using Different Models / ä½¿ç”¨ä¸åŒæ¨¡å‹

The framework supports three model types:
æ¡†æ¶æ”¯æŒä¸‰ç§æ¨¡å‹ç±»å‹:

1. **XGBoost** (Recommended / æ¨è)
   - Best performance / æœ€ä½³æ€§èƒ½
   - Supports true incremental training / æ”¯æŒçœŸæ­£çš„å¢é‡è®­ç»ƒ
   - Fast and efficient / å¿«é€Ÿé«˜æ•ˆ

2. **LightGBM** (Alternative / æ›¿ä»£æ–¹æ¡ˆ)
   - Similar to XGBoost / ç±»ä¼¼XGBoost
   - Faster training on large datasets / åœ¨å¤§æ•°æ®é›†ä¸Šè®­ç»ƒæ›´å¿«
   - Good memory efficiency / è‰¯å¥½çš„å†…å­˜æ•ˆç‡

3. **Gradient Boosting** (Scikit-learn)
   - No external dependencies / æ— å¤–éƒ¨ä¾èµ–
   - Limited incremental training support / å¢é‡è®­ç»ƒæ”¯æŒæœ‰é™
   - Good baseline / è‰¯å¥½çš„åŸºçº¿

### Custom Fine-tuning Strategy / è‡ªå®šä¹‰å¾®è°ƒç­–ç•¥

Two strategies are available:
æä¾›ä¸¤ç§ç­–ç•¥:

1. **Incremental Training** (Default / é»˜è®¤)
   - Continue training from general model / ä»é€šç”¨æ¨¡å‹ç»§ç»­è®­ç»ƒ
   - Add new trees with reduced learning rate / ä½¿ç”¨é™ä½çš„å­¦ä¹ ç‡æ·»åŠ æ–°æ ‘
   - Best for tree-based models / æœ€é€‚åˆåŸºäºæ ‘çš„æ¨¡å‹

2. **Correction Model** (Alternative / æ›¿ä»£æ–¹æ¡ˆ)
   - Train small model to correct general predictions / è®­ç»ƒå°æ¨¡å‹æ¥æ ¡æ­£é€šç”¨é¢„æµ‹
   - Final prediction = general + correction / æœ€ç»ˆé¢„æµ‹ = é€šç”¨ + æ ¡æ­£
   - More robust to overfitting / æ›´èƒ½æŠµæŠ—è¿‡æ‹Ÿåˆ

---

## Troubleshooting / æ•…éšœæ’é™¤

### Issue: Poor personalized model performance / é—®é¢˜: ä¸ªæ€§åŒ–æ¨¡å‹æ€§èƒ½å·®

**Possible causes / å¯èƒ½åŸå› :**
1. Too few calibration samples / æ ¡å‡†æ ·æœ¬å¤ªå°‘
2. Learning rate too high / å­¦ä¹ ç‡å¤ªé«˜
3. Overfitting on calibration data / åœ¨æ ¡å‡†æ•°æ®ä¸Šè¿‡æ‹Ÿåˆ

**Solutions / è§£å†³æ–¹æ¡ˆ:**
1. Increase `n_samples` in `DATA_SPLIT_CONFIG` / å¢åŠ æ ¡å‡†æ ·æœ¬æ•°
2. Reduce `learning_rate` in `FINE_TUNING_CONFIG` / é™ä½å­¦ä¹ ç‡
3. Enable early stopping / å¯ç”¨early stopping
4. Increase regularization (`reg_alpha`, `reg_lambda`) / å¢åŠ æ­£åˆ™åŒ–

### Issue: Models not saving / é—®é¢˜: æ¨¡å‹æœªä¿å­˜

**Solution / è§£å†³æ–¹æ¡ˆ:**
Check that output directories have write permissions:
æ£€æŸ¥è¾“å‡ºç›®å½•æœ‰å†™æƒé™:

```bash
# Create directories manually if needed / å¦‚éœ€è¦æ‰‹åŠ¨åˆ›å»ºç›®å½•
mkdir -p saved_models/general_models
mkdir -p saved_models/personalized_models
mkdir -p results_transfer_learning
```

### Issue: Memory error with large datasets / é—®é¢˜: å¤§æ•°æ®é›†å†…å­˜é”™è¯¯

**Solutions / è§£å†³æ–¹æ¡ˆ:**
1. Use LightGBM instead of XGBoost (more memory efficient) / ä½¿ç”¨LightGBMä»£æ›¿XGBoost
2. Reduce `n_estimators` in model config / å‡å°‘æ¨¡å‹ä¸­çš„æ ‘æ•°é‡
3. Process training files one at a time / é€ä¸ªå¤„ç†è®­ç»ƒæ–‡ä»¶

---

## Performance Tips / æ€§èƒ½æç¤º

### For Best Results / è·å¾—æœ€ä½³ç»“æœ:

1. **Use XGBoost** - Best accuracy and speed / æœ€ä½³å‡†ç¡®æ€§å’Œé€Ÿåº¦
2. **200 calibration samples** - Good balance / è‰¯å¥½å¹³è¡¡
3. **Early stopping enabled** - Prevents overfitting / é˜²æ­¢è¿‡æ‹Ÿåˆ
4. **Low learning rate (0.01)** - Stable fine-tuning / ç¨³å®šçš„å¾®è°ƒ

### For Fast Experimentation / å¿«é€Ÿå®éªŒ:

1. **Use fewer training files** - Faster general model training / æ›´å¿«çš„é€šç”¨æ¨¡å‹è®­ç»ƒ
2. **Reduce n_estimators** - Faster training / æ›´å¿«çš„è®­ç»ƒ
3. **Disable visualizations** - Set `save_figures=False` / ç¦ç”¨å¯è§†åŒ–

---

## Technical Details / æŠ€æœ¯ç»†èŠ‚

### How Transfer Learning Works / è¿ç§»å­¦ä¹ åŸç†

1. **General Model Phase / é€šç”¨æ¨¡å‹é˜¶æ®µ**
   - Train on diverse patient data / åœ¨å¤šæ ·åŒ–æ‚£è€…æ•°æ®ä¸Šè®­ç»ƒ
   - Learn general PLETH-BP relationships / å­¦ä¹ é€šç”¨çš„PLETH-BPå…³ç³»
   - Capture physiological patterns / æ•è·ç”Ÿç†æ¨¡å¼

2. **Personalization Phase / ä¸ªæ€§åŒ–é˜¶æ®µ**
   - Use small amount of patient-specific data / ä½¿ç”¨å°‘é‡æ‚£è€…ç‰¹å®šæ•°æ®
   - Fine-tune model parameters / å¾®è°ƒæ¨¡å‹å‚æ•°
   - Adapt to individual characteristics / é€‚åº”ä¸ªä½“ç‰¹å¾

3. **Key Advantages / å…³é”®ä¼˜åŠ¿**
   - Preserve general knowledge / ä¿ç•™é€šç”¨çŸ¥è¯†
   - Adapt to individual differences / é€‚åº”ä¸ªä½“å·®å¼‚
   - Avoid catastrophic forgetting / é¿å…ç¾éš¾æ€§é—å¿˜

### Why It Works Better Than Calibration / ä¸ºä»€ä¹ˆæ¯”æ ¡å‡†æ•ˆæœå¥½

- **Simple calibration**: Only adjusts output offset / ç®€å•æ ¡å‡†: åªè°ƒæ•´è¾“å‡ºåç§»
- **Transfer learning**: Adapts entire model / è¿ç§»å­¦ä¹ : é€‚é…æ•´ä¸ªæ¨¡å‹
- **Result**: Captures complex individual patterns / ç»“æœ: æ•è·å¤æ‚çš„ä¸ªä½“æ¨¡å¼

---

## Citation / å¼•ç”¨

If you use this framework in your research, please cite:
å¦‚æœåœ¨ç ”ç©¶ä¸­ä½¿ç”¨æ­¤æ¡†æ¶,è¯·å¼•ç”¨:

```
[Your citation information here]
```

---

## Support / æ”¯æŒ

For issues or questions:
é—®é¢˜æˆ–ç–‘é—®:

1. Check this README / æŸ¥çœ‹æœ¬README
2. Review `transfer_learning.md` for detailed design / æŸ¥çœ‹è¯¦ç»†è®¾è®¡æ–‡æ¡£
3. Check configuration files / æ£€æŸ¥é…ç½®æ–‡ä»¶
4. Review error messages and logs / æŸ¥çœ‹é”™è¯¯æ¶ˆæ¯å’Œæ—¥å¿—

---

## License / è®¸å¯

[Your license information]

---

**Happy Predicting! / é¢„æµ‹æ„‰å¿«!** ğŸ©ºğŸ“Š
